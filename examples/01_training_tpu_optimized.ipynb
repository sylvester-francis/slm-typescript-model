{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# TypeScript SLM Training on TPU (Optimized)\n\nThis notebook trains the TypeScript-SLM model using Google Colab TPUs.\n\n## Setup Instructions:\n1. **Select TPU Runtime**: Go to `Runtime > Change runtime type > TPU v2 or v5e`\n2. **Run the installation cell below and RESTART RUNTIME when prompted**\n3. **Upload your data**: Upload `train.jsonl` to Colab or mount Google Drive\n4. **After restart, skip the installation cell and run all other cells**\n\n**Optimized for TPU to avoid OOM errors with reduced batch size and sequence length.**"
  },
  {
   "cell_type": "code",
   "id": "g7u3b8tr19r",
   "source": "# STEP 1: Run this cell ONCE, then Runtime > Restart runtime\n# After restart, SKIP this cell and continue from the next cell\n\n# Install PyTorch and PyTorch/XLA for TPU\n!pip install torch torch-xla -f https://storage.googleapis.com/libtpu-releases/index.html\n\n# Install other dependencies\n!pip install -U transformers datasets peft accelerate trl wandb\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"✓ Installation complete!\")\nprint(\"=\"*70)\nprint(\"\\n⚠️  NEXT STEP: Runtime > Restart runtime\")\nprint(\"⚠️  After restart, SKIP this cell and run from the next cell\\n\")\nprint(\"=\"*70)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Verify TPU setup and import libraries\nimport os\nimport torch\n\nprint(f\"PyTorch version: {torch.__version__}\")\n\n# Try to import torch_xla\ntry:\n    import torch_xla\n    import torch_xla.core.xla_model as xm\n    device = xm.xla_device()\n    print(f\"✓ torch-xla version: {torch_xla.__version__}\")\n    print(f\"✓ TPU device: {device}\")\n    print(\"\\n✓ TPU is ready to use!\")\nexcept ImportError as e:\n    print(\"\\n❌ ERROR: torch-xla is not installed!\")\n    print(\"\\nPlease run the installation commands:\")\n    print(\"!pip install torch torch-xla -f https://storage.googleapis.com/libtpu-releases/index.html\")\n    print(\"!pip install -U transformers datasets peft accelerate trl wandb\")\n    print(\"\\nThen: Runtime > Restart runtime\")\n    raise\nexcept Exception as e:\n    print(f\"\\n❌ ERROR: {e}\")\n    print(\"\\nMake sure you selected a TPU runtime:\")\n    print(\"Runtime > Change runtime type > TPU\")\n    raise"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport torch\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    TrainingArguments,\n)\nfrom peft import LoraConfig\nfrom trl import SFTTrainer\n\n# Check if TPU is available\ntry:\n    import torch_xla.core.xla_model as xm\n    device = xm.xla_device()\n    print(f\"Using TPU: {device}\")\nexcept:\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    print(f\"Using device: {device}\")\n\n# Login to Hugging Face (optional)\nfrom huggingface_hub import login\n# login(token=\"YOUR_TOKEN\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "model_name = \"Qwen/Qwen2.5-Coder-1.5B-Instruct\"\n",
    "new_model = \"typescript-slm-1.5b\"\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"json\", data_files=\"train.jsonl\", split=\"train\")\n",
    "print(f\"Loaded {len(dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Model and Tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\ntokenizer.model_max_length = 1024  # Set max length for the tokenizer\n\n# NOTE: Removed device_map=\"auto\" for TPU compatibility\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=torch.bfloat16,\n    # device_map=\"auto\"  <-- REMOVED\n)\n\n# Enable gradient checkpointing to save memory\nmodel.gradient_checkpointing_enable()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA Configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Arguments - OPTIMIZED FOR MEMORY\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=1,      # Reduced from 4 to 1\n",
    "    gradient_accumulation_steps=16,     # Increased to maintain effective batch size\n",
    "    optim=\"adamw_torch\",\n",
    "    save_steps=100,\n",
    "    logging_steps=10,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    report_to=\"wandb\",\n",
    "    gradient_checkpointing=True,        # Explicitly enable\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Prepare formatting function for SFTTrainer\ndef formatting_func(example):\n    \"\"\"Format the dataset for training.\"\"\"\n    return example[\"text\"]\n\n# Trainer - Updated for latest TRL API\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset,\n    peft_config=peft_config,\n    processing_class=tokenizer,  # Use processing_class instead of tokenizer\n    args=training_arguments,\n    formatting_func=formatting_func,\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model\n",
    "trainer.model.save_pretrained(new_model)\n",
    "tokenizer.save_pretrained(new_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}